{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def dynamic_merge(TableName: str, Destination_Table: str, primary_key_list: str):\n",
    "    # Retrieve column names for both source and destination tables\n",
    "    source_columns = [col[0] for col in spark.sql(f\"DESCRIBE {TableName}\").collect()]\n",
    "    target_columns = [col[0] for col in spark.sql(f\"DESCRIBE {Destination_Table}\").collect()]\n",
    " \n",
    "    # Filter columns to only those in source_columns that are also in target_columns\n",
    "    matching_columns = [col for col in source_columns if col in target_columns]\n",
    " \n",
    "    if not matching_columns:\n",
    "        raise ValueError(\"No matching columns found between source and destination tables.\")\n",
    " \n",
    "    # Deduplicate the source table\n",
    "    deduplicated_view = \"deduplicated_source\"\n",
    "    dedup_query = f\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW {deduplicated_view} AS\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY CUSTOMER_ID\n",
    "                ORDER BY\n",
    "                    CASE\n",
    "                        WHEN ⁠ METADATA$ACTION ⁠ = 'DELETE' AND ⁠ METADATA$ISUPDATE ⁠ = FALSE THEN 1 -- High priority\n",
    "                        WHEN ⁠ METADATA$ISUPDATE ⁠ = TRUE THEN 2  -- Medium priority\n",
    "                        WHEN ⁠ METADATA$ACTION ⁠ = 'INSERT' AND ⁠ METADATA$ISUPDATE ⁠ = FALSE THEN 3 -- Low priority\n",
    "                    END, current_timestamp DESC\n",
    "            ) AS row_num\n",
    "        FROM CUSTOMER_Staging_Table\n",
    "    ) t\n",
    "    WHERE row_num = 1\n",
    "    \"\"\"\n",
    "    spark.sql(dedup_query)\n",
    "    print(\"Source table deduplicated successfully.\")\n",
    " \n",
    "    # Display the deduplicated source table\n",
    "    deduplicated_data = spark.sql(f\"SELECT * FROM {deduplicated_view}\")\n",
    "    deduplicated_data.show(truncate=False)\n",
    " \n",
    "    # Create temp views for counting purposes\n",
    "    spark.sql(f\"CREATE OR REPLACE TEMP VIEW to_delete AS SELECT * FROM {deduplicated_view} WHERE ⁠ METADATA$ACTION ⁠ = 'DELETE' AND ⁠ METADATA$ISUPDATE ⁠ = FALSE\")\n",
    "    spark.sql(f\"CREATE OR REPLACE TEMP VIEW to_update AS SELECT * FROM {deduplicated_view} WHERE ⁠ METADATA$ACTION ⁠ = 'INSERT' AND ⁠ METADATA$ISUPDATE ⁠ = TRUE\")\n",
    "    spark.sql(f\"CREATE OR REPLACE TEMP VIEW to_insert AS SELECT * FROM {deduplicated_view} WHERE ⁠ METADATA$ACTION ⁠ = 'INSERT' AND ⁠ METADATA$ISUPDATE ⁠ = FALSE\")\n",
    " \n",
    "    # Get counts before MERGE\n",
    "    deleted_count = spark.sql(\"SELECT COUNT(*) AS count FROM to_delete\").collect()[0][\"count\"]\n",
    "    updated_count = spark.sql(\"SELECT COUNT(*) AS count FROM to_update\").collect()[0][\"count\"]\n",
    "    inserted_count = spark.sql(\"SELECT COUNT(*) AS count FROM to_insert\").collect()[0][\"count\"]\n",
    " \n",
    "    # Construct the MERGE query dynamically\n",
    "    update_clause = \", \".join([f\"target.{col} = stream_stage.{col}\" for col in matching_columns])\n",
    "    insert_columns = \", \".join(matching_columns)\n",
    "    insert_values = \", \".join([f\"stream_stage.{col}\" for col in matching_columns])\n",
    " \n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {Destination_Table} AS target\n",
    "    USING {deduplicated_view} AS stream_stage\n",
    "    ON {\" AND \".join([f\"target.{key} = stream_stage.{key}\" for key in primary_key_list.split(\",\")])}\n",
    "    WHEN MATCHED AND stream_stage.⁠ METADATA$ACTION ⁠ = 'DELETE' AND stream_stage.⁠ METADATA$ISUPDATE ⁠ = FALSE THEN DELETE\n",
    "    WHEN MATCHED AND stream_stage.⁠ METADATA$ACTION ⁠ = 'INSERT' AND stream_stage.⁠ METADATA$ISUPDATE ⁠ = TRUE THEN\n",
    "        UPDATE SET {update_clause}\n",
    "    WHEN NOT MATCHED AND stream_stage.⁠ METADATA$ACTION ⁠ = 'INSERT' AND stream_stage.⁠ METADATA$ISUPDATE ⁠ = FALSE THEN\n",
    "        INSERT ({insert_columns})\n",
    "        VALUES ({insert_values})\n",
    "    \"\"\"\n",
    " \n",
    "    # Execute the merge statement\n",
    "    spark.sql(merge_sql)\n",
    "    print(\"MERGE operation completed successfully.\")\n",
    " \n",
    "    # Print counts\n",
    "    print(f\"Deleted rows: {deleted_count}\")\n",
    "    print(f\"Updated rows: {updated_count}\")\n",
    "    print(f\"Inserted rows: {inserted_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dynamic_merge(TableName, Destination_Table, primary_key_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
